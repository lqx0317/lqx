# 一、机器学习介绍
## 1.机器学习概念
## 2.机器学习分类体系（监督学习？）
### 2.1.生成模型vs判别模型
- 数据挖掘「分类」技术中统计模型被分为产生成式和判别式两大类，「生成式模型」学习的是「联合概率」，主要侧重各类数据的分布情况；「判别式模型」学习的是「条件概率」，主要侧重各类数据的分类边界。

#### （1）生成模型
在生成式模型中通过统计来建模 p(x|y)，再通过bayes公式来判断类标。通过训练集，计算联合概率 p(x,y)，由联合概率公式有：
$$p(x,y)=p(y)p(x|y)=p(y)$$

#### （2）判别模型

### 2.2.概率模型vs非概率模型
#### （1）前言
- 如果将机器学习算法的输入、输出数据看做随机变量，就可以用概率论的观点对问题进行建模。
- 概率模型和非概率模型两者的本质区别在于是否涉及到概率分布。
- 概率模型：凡是涉及到求解 P(X,Y)或者P(Y|X)，都是概率模型。逻辑回归、高斯判别分析、朴素贝叶斯都属于概率模型。
- 非概率模型：凡是只涉及到假设空间 H 的，都是非概率模型。非概率模型都是判别模型。感知机、支持向量机、神经网络、K近邻都属于非概率模型。

### 2.3.参数模型vs非参数模型

# 二、机器学习

机器学习符号表

<center>

符号|含义|备注
-|-|-
m|样本大小|
h|函数或者模型|模型，假设
x|标量值
$\mathbf{x}$|向量
D|数据集
y|实际值
$\hat y$|预测值
$\pmb{X}$|矩阵
$\mathrm{X}$|随机变量
$\mathcal{X}$|训练样本
\|x\||x的绝对值，向量x的长度，集合x的势（集合的元素个数）


</center>

## （零）机器学习思维
### 1.训练机器学习思维

## （一）机器学习基础
### 1.什么是机器学习
- 机器学习就是让机器具备找一个函数的能力。找到一个函数以后，就能处理输入产生输出。语音识别：$f(声音)=“how are you”$。图像识别：$f(图片)=“cat”$。不同的函数类型：回归Regression，输入数字，输出是一个数值。分类Classification，输入类别，输出类别。生成Strucured Learning，生成图片，文档。如何寻找函数。首先依据领域知识假设一个f。

### 2.机器学习基础概念
#### （1）机器学习
机器学习是一种基于已知数据，根据假设模型，训练得到一个确定函数的过程。在得到确定函数之后，输入新的数据，输出预测结果。
##### 按预测属性类型
- 分类任务
- 回归任务

##### 按学习方式
- 监督学习<br>
- 无监督学习
- 半监督学习
- 强化学习

#### （2）向量
默认是列向量。
#### （3）属性/特征
特征，也叫属性。在数据中的表现形式为数据集的一列，一个特征表示一列数据。
##### 符号
我们可以用上标表示属性，例如 $x^1,x^2,...,x^d$，我们可以用双下标表示属性，例如用 $x_{i1},...,x_{id}$ 表示某一个样本的属性 1,...,d。
#### （4）属性值/特征值
表示特征的具体取值
#### （5）属性空间/样本空间
##### 定义
特征的所有可能取值组成的集合。如果特征是一维的，则特征空间是一个向量，如性别的取值组合是[0,1]。如果特征是多维的，则特征空间是多维度的集合，如一个人有性别和国籍组合，特征空间表示为[[0,1],[China,Japan,Korea,America]]。属性张成的空间，称为属性空间或样本空间。例如西瓜书中我们把“色泽”、“根蒂”、“声响”作为三个坐标轴，把他们张成一个用于描述西瓜的三维空间，每个西瓜都可以在这个空间中找到自己的坐标位置，由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个“特征向量”。
#### （6）样本/特征向量
##### 定义
样本，又叫记录、实例，不包括标签。具体是指一个事物的各个具体属性描述。
##### 符号
样本一般用特征向量表示，比如第 j 条样本：$\mathbf{x_j} = (x_j^1,x_j^2,...,x_j^d)^T$。
该样本有 d 维，第 j 个样本的第 i 个特征值为 $x_j^i$。
#### （7）样本空间/属性空间
#### （8）标签
在有监督学习中，标签是我们要预测的变量，如 y 变量。在已知的数据集中，目标列称为标签label。它可以在模型学习过程中进行指导。
#### （9）标签值
标签值是标签的具体取值。
#### （10）标签空间
标签所有可能的取值组成的集合。
#### （11）样例
##### 定义
样本和标签组成一个样例，即拥有了标签的实例，称为样例。
##### 符号
$(\mathbf{x}_j,y_j)$，其中$\mathbf{x}_j = (x_j^1,...,x_j^d)^T$
#### （12）样例空间
#### （13）数据集
样例的集合。
##### 符号
我们用大写字母 $D$ 表示数据集，数据集是一个集合，$D = \{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),...,(\mathbf{x}_m,y_m)\}$
数据集分为：
- 训练集<br>
训练集用于训练和确定模型参数。
- 验证集<br>
验证集用于模型选择，帮助选出最好的模型。
- 测试集<br>
测试集用于评估模型，测试模型用于新样本的能力（即泛化能力）。模型在训练集表现好并不能说明全部问题，在测试集表现好才是好的模型。

#### （14）模型/假设
##### 定义
模型（model），带有未知参数（unknown parameter）的函数（function），一组函数，函数集合，模型也叫假设（hypothesis）。机器学习一开始一般都需要假设一个模型来拟合数据。
##### 符号

#### （15）参数
#### （16）训练
当我们有了一个假设模型之后，我们要基于数据集训练出一个函数（求参数），这个函数能够很好地映射输入（自变量）和输出（因变量）的关系。
#### （17）函数

#### （18）真相
数据中真正存在的潜在规律
#### （19）假设
学习模型对应了关于数据的某种潜在规则


#### （6）收敛
- 收敛是一个数学名词，是研究函数的一个重要工具，是指汇聚于一点，向某一值靠近。收敛类型有收敛数列、函数收敛、全局收敛、局部收敛。
- 在数学分析中，与收敛相对的概念就是发散。收敛与发散判断方法简单来说就是有极限（极限不为无穷）就是收敛，没有极限就是发散。简单来说就是看极限是否存在。
- 一个迭代算法（迭代是循环的一次执行），随着迭代的进行，输出越来越接近一些特定值就是收敛。
- 机器学习模型不收敛的直观体现是loss函数无法下降。不论是loss值一直大幅震荡还是loss值一直居高不下，都表示模型不收敛。
3.领域知识
4.参数
weight，bias
5.loss损失函数
损失函数 $L(b,w)$ 是关于参数的函数。输入是模型model里的参数，输出是
6.估测值 $y$
7.真实值 $\hat y$

### 3.评价指标

## （二）数据处理
## （三）机器学习模型
### 1.回归分析
- 回归分析是确定两种或两种以上变量间（因变量也可能是多个变量）相互依赖的定量关系的一种统计分析方法，是一种预测性的建模技术，它研究的是因变量和自变量之间的关系，例如中国人的消费习惯对美国经济的影响等。其又分为线性回归分析和非线性回归分析。
- 回归分析是广泛使用的统计分析方法，可以用于分析其中一方对另一方的影响关系，也可以分析自变量对因变量的影响方向（正向影响还是负向影响）。
- 回归分析是通过建立因变量Y和影响它的自变量$X_1,...,X_n$ 间的回归模型，衡量自变量 $X_i$ 对因变量的影响能力，进而用来预测因变量的发展趋势。回归分析模型包括线性回归和非线性回归两种。线性回归又分为简单线性回归、多重现性回归。非线性回归需要通过对数转化等方式，将其转化为线性回归的形式进行研究。

#### ➤➤➤ 渊源 ➤➤➤
#### （1）高尔顿
- 相关和回归分析的创始人
- 发现父亲的身高和儿子的身高之间存在着某种给定的关系：事实上子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和。
- 高尔顿回归的不足，没有引入控制变量。从结果上来看，回归模型更像是显示了两个变量的统计关联度，而非因果关系。
- 向平均回归：人类的身高从高矮两个极端移向所有人的平均值。
#### （2）高尔顿谬误
如果人类的身高一代又一代都向中等平庸回归，那么人类的身高必将趋于相等。而在经济学上也有相同的谬误，经济发展水平较低的国家和地区将会有较高的经济增长率，而经济发展水平较高的国家和地区则只有较低的经济增长率。而现实是世界各国之间的经济发展的差距并没有缩小。为什么看上去合理的假设过程导出的结论却是谬论呢？是因为各人身高的长成是一个充满了偶然的、随机的、不确定因素的过程，虽然各人的身高主要决定于父母的遗传基因，但是在生命的形成过程中却充满了各种各样的机会变异，这些机会变异对各人身高的决定也有相当的作用。高尔顿发现的人类身高”向中等平庸回归“的现象，纯粹是由于机会变异所产生的回归效应所致，它并不会导致人类升高趋于相同。

以上机会变异就是扰动项，也就是误差？现实世界是充满机会变异和不确定性的世界，不能完全用想象的确定性世界的思维模式来进行思考和决策。

#### （3）控制变量
实验中主要涉及三种变量：自变量、因变量和控制变量。自变量和因变量称为实验变量。
- 自变量：在实验中由实验者操作和控制的变量
- 因变量：实验中被试对自变量操作反应的实验反应值
- 控制变量：指那些除了实验因素（自变量）以外的所有影响实验结果的变量，这些变量不是本实验所要研究的变量。只有将自变量以外一切能引起因变量变化的变量控制好，才能弄清实验中的因果关系。控制变量法是指把多因素的问题变成多个单因素的问题，而只改变其中的某一个因素。从而研究这个因素对事物影响，分别加以研究。

#### （4）尤勒
- 在高尔顿的基础上提出了回归模型中应当加入尽可能多的控制变量的理念。


#### ➤➤➤ 线性回归 ➤➤➤
#### （1）线性回归
线性回归是一种以线性模型来建模自变量和因变量关系的方法。当自变量只有一个的情况称为简单线性回归，当自变量大于一个的情况称为多重线性回归。

#### （2）线性回归模型
形式化地表示，给定一个数据集$\{y_i,x_{i1},...,x_{ip}\}_{i=1}^n$，其中 n 为样本个数，线性回归会假设自变量 $X = x_1,...,x_p$ 和因变量 y 之间的关系是线性的，并且通过加入误差变量 $\epsilon$ 来建模 x 以外的因素对 y 的印象。模型形式如下：
$$y_i = X_i^T\beta +\epsilon_i$$
其中，$\beta$即为模型参数，通常可以使用最小二乘法估计得到。

###### 总体回归函数
$$E(Y|X) = f(X) = \beta_0 + \beta_1X$$
###### 总体回归模型
$$Y=f(X)+\mu=\beta_0+\beta_1X+\mu$$
###### 样本回归函数
$$\hat Y = \hat{\beta_0} + \hat{\beta_1}X$$
###### 样本回归模型
$$Y=\hat Y+\hat\mu=\hat{\beta_0} + \hat{\beta_1}X+\epsilon$$
#### （3）线性回归应用
- 预测：线性回归可以在拟合到已知数据集后用于预测自变量所对应的因变量。
- 解释：线性回归可以用于量化因变量与自变量之间关系的强度。

#### （4）检验
建立模型的步骤：理论模型设计 -> 样本数据收集 -> 模型参数估计 -> 模型的检验

#### （1）正态性
###### 假设
线性回归模型的因变量假定来源于正态分布的总体。在这一假定前提下，通常的做法是采用极大似然法或最小二乘法给出参数的估计。如果通过随机抽样的数据非正态分布，则会使显著性校验程序是无效的，实际上估计出来的参数没有意义，直接影响回归分析结果。因此，非正态数据的处理是应用线性回归模型时需要解决的问题。

**线性回归**要求因变量 Y 值满足满足正态分布要求，如果不满足如何办呢？一般情况下可对因变量进行数据转换处理，如对数、或者开根号等，使数据尽可能的满足正态性要求。




### 1.决策树
#### （1）节点
![决策树](img/决策树1.jpg)
- 作为一种常见的解决分类问题的模型，决策树的形状类似于一个树，包含根节点、中间节点和叶子节点。
- 每个叶子节点存放一个类别，每个非叶子节点标识一个特征属性上的测试。根节点代表总体样本，中间节点表示一个属性上测试得到的样本子集，叶子节点存放标签。
- 决策树决策过程就是从根节点开始，测试待分类项中相应的属性，并按照其值选择输出分支，直至到达叶子节点，将叶子节点存放的类别作为决策结果。
- 根节点、中间节点、叶子节点、决策节点、子节点。

#### （2）选择分裂属性
决策树分类属性的选择就是选择哪个自变量作为树杈，即在 n 个自变量中，优先选择哪个自变量进行分叉，而采用何种计算方式选择树杈决定了决策树算法的类型。
典型的分裂属性的选择方法有ID3算法、C4.5算法、CART算法三种。三种决策树算法选择树杈的方式是不一样的。
#### （3）熵
熵是衡量数据集纯度的指标。
$$E(D) = -\sum_{i=1}^mp_i\log_2p_i$$
- D 表示（分裂前的）数据集。E(D) 就是数据集的熵。
- m 表示目标属性有 m 个值，$i=1,2,...,m$，$m$为类别数。
- $p_i=\frac{|C_i|}{n}$，$|C_i|$ 指类别为 $C_i$ 的样本数，n 表示样本大小，$p_i$ 表示类别 $C_i$ 的样本占比。

例如：
如果有一个大小为10的布尔值样本集$S_b$，其中有6个真值，4个假值。那么该样本的熵为：
$$Entropy(S)=-\frac{6}{10}\log_2\frac{6}{10}-\frac{4}{10}\log_2\frac{4}{10}=0.9710$$
样本集的熵越高，则样本集的纯度越低。样本集的纯度越高，样本集的熵越低。以上例子，样本集的熵为0.9710，已经很高了，说明样本集的纯度很低，再看样本集，一共10个样本，6个真值，4个假值，纯度缺失很低。如果有5个真值，5个假值，样本集的熵为1。如果有10个真值，0个假值，样本集的熵为0。
#### （4）信息增益
信息增益是分支属性对于数据集分类好坏程度的度量。

信息增益 = 分裂前后数据集熵降低的值 = 分裂前数据集的熵 - 分裂后数据集的熵
$$gain_A(D)=E(D) - E_A(D)$$
- E(D) 分裂前数据集的熵
- $E_A(D)$ 属性 A 对数据集 D 划分（分裂）后的熵（子集熵的和）
- $gain_A(D)$ 是属性 A 分裂数据集 D 后的信息增益
$$E_A(D)=\sum_{j=1}^v{\frac{|D_j|}{|D|}E(D_j)}$$
- $E_A(D)$ 表示分裂后数据集的熵（数据子集的熵的和）
- j=1,2,...,v 表示分裂后的分支个数
- $|D_1|,|D_2|,...,|D_v|$ 表示分裂后的各个分支的数据子集的样本数。$|D|$ 是分裂前数据集的样本数。
- $E(D_1),E(D_2),...,E(D_v)$ 表示分裂受各个分支的数据子集的熵。

#### （5）ID3算法
- 哪个属性分裂数据集的信息增益最大，ID3就选择哪个属性分裂数据集
- ID3算法的不足之处是只能选择离散型属性进行分裂，即仅支持分裂的属性是离散值，信息增益的选择分裂属性的方式会偏向选择具有大量值的属性
- ID3算法是目前决策树算法中较有影响力的算法，1986年提出，该算法只是一个启发式的算法。
- ID3算法的核心是判断测试哪个属性为最佳的分类属性。ID3算法选择分裂后信息增益最大的属性进行分裂，以信息增益度量属性选择。
- ID3算法是在每个节点处选取能获得最高信息增益的分支属性进行分裂
- 在每个决策节点划分分支，选取分支属性的目的是将整个决策树的样本纯度提升
- 衡量样本集纯度的指标是熵，计算分支属性对于样本集好坏程度的度量是信息增益
- 由于分裂后样本集的纯度提高，样本集的熵降低，熵降低的值就是该分裂方法的信息增益
- 以某个属性作为分支属性时能获得最大的信息增益，则认为该属性具有最强的区分样本的能力

#### （6）信息增益率
$$gain\_ratio_A(D) = \frac{gain_A(D)}{split_A(D)}$$
- $gain\_ratio_A(D)$ 表示属性 A 分裂数据集 D 的信息增益率
- $gain_A(D)$ 表示属性 A 分裂数据集 D 的信息增益
- $split_A(D)$表示属性 A 分裂数据集 D 的分裂信息

#### （7）分裂信息
$$split_A(D) = -\sum_{j=1}^v\frac{|D_j|}{|D|}\log_2\frac{|D_j|}{|D|}$$
- $split_A(D)$表示属性 A 分裂数据集 D 的分裂信息
- $j=1,j=2,...,j=v$ 表示分裂后的分支数目
- $|D_1|,|D_2|,...,|D_v|$表示分裂后各个分支，即数据子集的样本数
- $|D|$ 是分裂前的数据集的样本数

#### （8）C4.5算法
- C4.5算法采用最大信息增益率的属性被选为分裂属性
- C4.5支持离散型和连续性属性，即分裂的属性可以是离散型也可以是连续性。
- 对于分裂属性为连续型时，只需将连续型变量由小到大递增排序，取中点作为分裂点，然后按照离散型变量计算信息增益的方法计算信息增益
- C4.5只是ID3算法的改进算法

#### （9）基尼系数
$$Gini(D)=1-\sum_{i=1}^mp_i^2$$
- Gini(D) 表示数据集 D 的基尼系数
- i=1,i=2,...,i=m 表示目标属性有 m 个分类
- $p_i=\frac{|C_i|}{|D|}$。$|C_1|,|C_2|,...,|C_m|$表示目标属性各个类别的样本数，$|D|$ 是总样本数。

基尼值越小，数据集的纯度越高。
属性 A 分裂数据集的基尼系数定义为：
$$Gini_A(D) = \sum_{j=1}^v\frac{|D_j|}{|D|}Gini(D_j)$$
- $Gini_A(D)$ 表示属性 A 划分数据集得到的基尼系数和
- j=1,j=2,...,j=v 表示分支个数，$|D_1|,|D_2|,...,|D_v|$表示各个分支数据子集的样本数，|D| 是分裂前样本数。
- $Gini(D_1),Gini(D_2),...,Gini(D_v)$表示各个分支数据子集的基尼系数 

#### （10）CART算法
- CART算法在候选属性集合A中选择那个使得划分后基尼系数最小的属性作为最优划分属性。
- CART是个二叉树，也就是当使用某个特征划分样本集合只有两个集合：等于给定的特征值的样本集合D1和不等于给定的特征值的样本集合D2。
CART全称是分类与回归树，通过构建二叉树达到预测目的，是一种十分有效的非参数分类和回归算法。

#### （3）树剪枝

### 2.集成学习
#### （1）集成学习
集成学习就是将若干个弱分类
#### （2）弱学习器
#### （3）强学习器
#### （4）bagging
#### （5）boosting
- 样本选择：bagging算法是有放回的随机采样。boosting算法是每一轮训练集不变，只是训练集众的每个样例在分类器中的权重发生变化，
#### （6）stack

## （四）XGBoost
- XGBoost（Extreme Gradient Boosting，极限梯度提升）用来取代回归模型（线性回归、逻辑回归）。
- XGBoost基于决策树的集成机器学习算法，以梯度提升（Gradient Boost）为框架。
- 在非结构化数据（图像、语音、文本）的预测问题中，神经网络表现优于其他算法或框架，但在处理中小型结构数据时，基于决策树的算法是最好的。
- 决策树演变：Decision Trees-->Bagging-->Random Forest-->Boosting-->Gradient Boosting-->XGBoost
- XGBoost特点：应用范围广泛，可以解决回归、分类、排序以及用户自定义的预测问题；可移植性，可以在windows、Linux、OS X上运行；语言，支持C++、python、R、Java、scala等机会所有主流编程语言；云集成，支持AWS、Yarn集群，可以很好的配合Flink、Spark等其他生态系统。

##### 决策树及其演变类比生活实例（面试场景）
- 决策树：每一名面试官都有一套自己的面试标准，比如教育水平、工作经验以及面试表现等。决策树类似于面试官根据他自己的标准面试求职者。
- 袋装法（Bagging）：现在面试官不止一人，而是一整个面试小组，小组中的每位面试官都有投票权。Bagging（Boostrap Aggregating）就是通过民主投票过程，综合所有面试官的投票，然后做出最终决定。
- 随机森林（Random Forest）：这是基于Bagging的算法，但与Bagging有明显的区别，它随机选择特征子集。也就是，每位面试官只会随机选择一些侧面来对求职者进行面试（比如测试编程技能的技术面或者是评估非技术技能的行为面试）
- Boosting：这是一种替代方法，每位面试官根据前一位面试官的反馈来调整评估标准。通过部署更动态的评估流程来提升面试效率。
- 梯度提升（Gradient Boosting）：这是Boosting的特例，这种算法通过梯度下降来最小化误差。用面试类比的话，就是战略公司用案例面试来剔除那些不符合要求的求职者。
- XGBoost：极限梯度提升。他可以在最短时间内用更少的计算资源得到更好的结果。


### 1.概念
#### （1）集成学习
- 集成学习的基本思想是把多个学习器通过一定方法进行组合，以达到最终效果的提升。

#### （2）弱学习算法
- 识别准确率仅比随机猜测高一些的学习算法为弱学习算法。
- 我们只需先找到一个弱学习算法，再将其提升为强学习算法，而不用一开始就找强学习算法。

#### （3）强学习算法
- 识别准确率很高并能在多项式时间内完成的学习算法称为强学习算法。

#### （4）Boosting
Boosting会训练一系列的弱学习器，并将所有学习器的预测结果组合起来作为最终预测结果，在学习过程中，后期的学习器更关注先前学习器学习中的错误。
- AdaBoosting<br>
1995年提出AdaBoost，继承了Boosting的思想，为每个弱学习器赋予不同的权重，将所有弱学习器的权重和作为预测的结果，达到强学习器的效果。
- Gradient Boosting<br>
1999年提出，与AdaBoost不同的是，它将损失函数梯度下降的方向作为优化的目标，新的学习器建立在之前学习器损失函数梯度下降的方向。代表算法有GBDT、XGBoost。

#### （5）Bagging
#### （6）Stacking

## （五）机器学习哲学
### 1.机器学习理论
#### （1）没有免费的午餐
- “某个模型不可能在每个方面都是最好的。” --所以要尝试许多模型，找到适合当前数据的
- “某个模型不会在每个方面都比另一个模型强”
- “你不可能在没有假设的情况下从数据中学习”

#### （2）奥卡姆剃刀
- “简单的是最好的”
- 好的公式应当是简洁明了的


# 三、机器学习算法

## ➤➤➤ 集成学习 ➤➤➤
## 1.集成学习
### 1.1.概念
#### （1）什么是集成学习
- 机器学习的求解过程可以看做是在假设空间搜索一个具有强泛化能力和高鲁棒性的学习模型，而在假设空间中寻找合适模型的过程是较为困难的。集成学习作为一类组合优化的学习方法，不仅能通过组合多个简单模型以获得一个性能更优的组合模型，而且允许研究者可以针对具体的机器学习问题设计组合方案以得到更为强大的解决方案。
- 集成学习能高效的解决各种机器学习问题

#### （2）集成学习思想
- 实际上，通过集成学习思想进行决策在文明社会开始时就已经存在了。例如，在民主社会中，公民们通过投票来选举官员或制定法律，对于个人而言，在重大医疗手术前通常咨询多名医生。这些例子表明，人们需要权衡并组合各种意见来做出最终的决定。其实，研究人员使用集成学习的最初目的和人们在日常生活中使用这些机制的原因相似。

### 1.2.集成学习发展历史
### 1.3.集成学习应用场景
### 1.4.集成学习未来趋势
### 1.5.集成学习设计策略

## ➤➤➤ 决策树 ➤➤➤

## ➤➤➤ 回归算法 ➤➤➤

## ➤➤➤ 贝叶斯 ➤➤➤

## ➤➤➤ 聚类 ➤➤➤
## 1.K-Means

## ➤➤➤ 基于核的算法 ➤➤➤
## 1.支持向量机SVM
## 2.线性判别分析LDA

## ➤➤➤ 决策树 ➤➤➤